# malrob/train.py
from __future__ import annotations

import json
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Dict, List, Tuple

import joblib
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


from malrob.model import BinaryNNConfig, BinaryResidualMLP, make_rf_binary


# -----------------------------
# Config
# -----------------------------
@dataclass
class BinaryTrainConfig:
    seed: int = 42
    test_size: float = 0.2
    batch_size: int = 128

    # NN training
    epochs: int = 120
    lr: float = 7e-4
    weight_decay: float = 5e-5
    tmax: int = 120  # CosineAnnealingLR T_max

    # Threshold selection
    threshold_method: str = "youden"  # "youden" only for now

    # Numerical stability / reproducibility knobs
    clip_scaled: float = 10.0  # clamp scaled features to [-clip_scaled, +clip_scaled]
    skip_nonfinite_batches: bool = True  # skip batches that yield non-finite loss


# -----------------------------
# Helpers
# -----------------------------
def probs_guard(probs: np.ndarray) -> np.ndarray:
    """Replace NaN/inf probs and clip to [0,1]."""
    p = np.asarray(probs, dtype=float)
    bad = ~np.isfinite(p)
    if bad.any():
        print(f"[WARN] Found {int(bad.sum())} non-finite probs; replacing with 0.5")
        p[bad] = 0.5
    return np.clip(p, 0.0, 1.0)


def youden_threshold(y_true: np.ndarray, probs: np.ndarray) -> float:
    """
    ROC-based threshold using Youdenâ€™s J = TPR - FPR.
    Handles the common 'inf' threshold returned by roc_curve.
    """
    y = np.asarray(y_true).astype(int)
    p = probs_guard(probs)

    fpr, tpr, thr = roc_curve(y, p)

    # roc_curve frequently includes an initial threshold=inf; drop non-finite thresholds
    finite = np.isfinite(thr)
    fpr, tpr, thr = fpr[finite], tpr[finite], thr[finite]

    j = tpr - fpr
    best_idx = int(np.argmax(j))
    return float(thr[best_idx])


def save_binary_eval(
    out_dir: Path,
    model_name: str,
    tag: str,
    y_true: np.ndarray,
    probs: np.ndarray,
    threshold: float,
) -> Dict:
    """
    Saves:
      - classification_report txt/json
      - confusion_matrix csv/png
      - meta json
    Also displays the confusion-matrix plot inline in notebooks.
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    base = f"{model_name}_{tag}"

    p = probs_guard(probs)
    pred = (p >= threshold).astype(int)

    report_txt = classification_report(y_true, pred, digits=4, zero_division=0)
    report_dict = classification_report(
        y_true, pred, digits=4, output_dict=True, zero_division=0
    )

    cm = confusion_matrix(y_true, pred)
    cm_df = pd.DataFrame(
        cm,
        index=["Benign (0)", "Malware (1)"],
        columns=["Pred Benign", "Pred Malware"],
    )

    # Print (matches notebook output expectations)
    auc = float(roc_auc_score(y_true, p))
    acc = float((pred == y_true).mean())
    print(f"\n[{model_name} | {tag}] threshold={threshold:.4f} acc={acc:.4f} auc={auc:.4f}")
    print(report_txt)
    print("\nConfusion matrix:")
    print(cm_df)

    # Save files
    (out_dir / f"{base}_classification_report.txt").write_text(report_txt, encoding="utf-8")
    with open(out_dir / f"{base}_classification_report.json", "w", encoding="utf-8") as f:
        json.dump(report_dict, f, indent=2)

    cm_df.to_csv(out_dir / f"{base}_confusion_matrix.csv", index=True)

    meta = {
        "model": model_name,
        "tag": tag,
        "threshold": float(threshold),
        "accuracy": acc,
        "auc": auc,
        "n_samples": int(len(y_true)),
    }
    with open(out_dir / f"{base}_meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

    # Confusion matrix PNG (matplotlib only; no seaborn)
    import matplotlib.pyplot as plt

    fig, ax = plt.subplots(figsize=(5, 4))
    ax.imshow(cm, interpolation="nearest")

    ax.set_title(f"{model_name} - Confusion Matrix ({tag})")
    ax.set_xlabel("Predicted Label")
    ax.set_ylabel("True Label")
    ax.set_xticks([0, 1])
    ax.set_xticklabels(["Benign", "Malware"])
    ax.set_yticks([0, 1])
    ax.set_yticklabels(["Benign", "Malware"])

    # annotate cells
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, str(cm[i, j]), ha="center", va="center")

    fig.tight_layout()
    png_path = out_dir / f"{base}_confusion_matrix.png"
    fig.savefig(png_path, dpi=300, bbox_inches="tight")
    print(f"[INFO] Saved confusion matrix PNG: {png_path}")

    # Show inline (Jupyter)
    plt.show()
    plt.close(fig)

    return meta


# -----------------------------
# Training
# -----------------------------
def train_rf_binary(X_train: pd.DataFrame, y_train: pd.Series, cfg: BinaryTrainConfig):
    rf = make_rf_binary(random_state=cfg.seed)
    print("[INFO] RF training started")
    rf.fit(X_train, y_train)
    print("[INFO] RF training completed")
    return rf


def train_nn_binary(
    X_train_scaled: np.ndarray,
    y_train: np.ndarray,
    cfg: BinaryTrainConfig,
    input_dim: int,
    device: str = "cpu",
) -> Tuple[nn.Module, List[float]]:
    torch.manual_seed(cfg.seed)
    np.random.seed(cfg.seed)

    model = BinaryResidualMLP(BinaryNNConfig(input_dim=input_dim)).to(device)

    X_t = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)
    y_t = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32, device=device)

    ds = torch.utils.data.TensorDataset(X_t, y_t)
    loader = torch.utils.data.DataLoader(ds, batch_size=cfg.batch_size, shuffle=True)

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.tmax)

    print("[INFO] NN training started")
    history: List[float] = []

    for epoch in range(1, cfg.epochs + 1):
        model.train()
        running = 0.0
        seen = 0

        for xb, yb in loader:
            # clamp in scaled space to avoid exploding activations from outliers
            if cfg.clip_scaled is not None:
                xb = torch.clamp(xb, -cfg.clip_scaled, cfg.clip_scaled)

            optimizer.zero_grad(set_to_none=True)
            logits = model(xb)  # (N,1)
            loss = criterion(logits, yb)

            if cfg.skip_nonfinite_batches and (not torch.isfinite(loss)):
                continue

            loss.backward()
            optimizer.step()

            bs = xb.size(0)
            running += float(loss.detach().cpu()) * bs
            seen += bs

        scheduler.step()
        epoch_loss = (running / max(seen, 1))
        history.append(epoch_loss)

        if epoch % 10 == 0 or epoch == 1:
            lr_now = optimizer.param_groups[0]["lr"]
            print(f"[NN] Epoch {epoch:03d}/{cfg.epochs} | loss={epoch_loss:.6f} | lr={lr_now:.6f}")

    print("[INFO] NN training completed")
    return model, history


# -----------------------------
# Pipeline
# -----------------------------
def train_binary_pipeline(data_csv: Path | str, out_dir: Path | str, cfg: BinaryTrainConfig) -> Dict:
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    data_csv = Path(data_csv)
    print(f"[INFO] Loading: {data_csv}")

    df = pd.read_csv(data_csv)
    if "label" not in df.columns:
        raise ValueError("CSV must contain a 'label' column.")

    X = df.drop(columns=["label"])
    y = df["label"].astype(int)

    # Keep feature names (prevents sklearn warnings later)
    feature_cols = X.columns.tolist()

    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=cfg.test_size,
        random_state=cfg.seed,
        stratify=y,
    )

    # Robustly handle non-finite raw values before scaling
    X_train_np = X_train.to_numpy(dtype=float)
    X_test_np = X_test.to_numpy(dtype=float)

    bad_train = ~np.isfinite(X_train_np)
    bad_test = ~np.isfinite(X_test_np)
    if bad_train.any() or bad_test.any():
        print(
            f"[WARN] Non-finite values in raw features. "
            f"train_bad={int(bad_train.sum())}, test_bad={int(bad_test.sum())}. Replacing with 0."
        )
        X_train_np[bad_train] = 0.0
        X_test_np[bad_test] = 0.0

    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train_np)
    X_test_s = scaler.transform(X_test_np)

    # Train RF on DataFrame with feature names (no sklearn warnings)
    rf = train_rf_binary(X_train[feature_cols], y_train, cfg)
    joblib.dump(rf, out_dir / "rf_binary_12k.joblib")
    joblib.dump(scaler, out_dir / "scaler_full_12k.joblib")

    # Train NN on scaled numpy
    nn_model, nn_history = train_nn_binary(
        X_train_scaled=X_train_s,
        y_train=y_train.values,
        cfg=cfg,
        input_dim=X_train_s.shape[1],
        device="cpu",
    )
    torch.save(nn_model.state_dict(), out_dir / "nn_binary_12k.pt")

    # Evaluate on clean test set
    # RF probs (DataFrame slice preserves feature names)
    rf_probs = rf.predict_proba(X_test[feature_cols])[:, 1]
    rf_probs = probs_guard(rf_probs)

    # NN probs
    nn_model.eval()
    with torch.no_grad():
        X_test_t = torch.tensor(X_test_s, dtype=torch.float32)
        nn_logits = nn_model(X_test_t).squeeze(1)
        nn_probs = torch.sigmoid(nn_logits).cpu().numpy()
    nn_probs = probs_guard(nn_probs)

    # Thresholds
    if cfg.threshold_method.lower() != "youden":
        raise ValueError("Only threshold_method='youden' is supported in this pipeline.")

    thr_rf = youden_threshold(y_test.values, rf_probs)
    thr_nn = youden_threshold(y_test.values, nn_probs)

    # Save thresholds
    artifacts_dir = out_dir / "artifacts"
    artifacts_dir.mkdir(parents=True, exist_ok=True)
    with open(artifacts_dir / "clean_thresholds.json", "w", encoding="utf-8") as f:
        json.dump({"clean_thr_nn": float(thr_nn), "clean_thr_rf": float(thr_rf)}, f, indent=2)

    # Save evaluations (prints + shows + saves PNGs)
    results_dir = out_dir / "results"
    rf_meta = save_binary_eval(
        out_dir=results_dir,
        model_name="RF_BINARY",
        tag="clean",
        y_true=y_test.values,
        probs=rf_probs,
        threshold=thr_rf,
    )
    nn_meta = save_binary_eval(
        out_dir=results_dir,
        model_name="NN_BINARY",
        tag="clean",
        y_true=y_test.values,
        probs=nn_probs,
        threshold=thr_nn,
    )

    # Summary
    summary = {
        "data_csv": str(data_csv),
        "out_dir": str(out_dir),
        "cfg": asdict(cfg),
        "n_train": int(len(X_train)),
        "n_test": int(len(X_test)),
        "n_features": int(X_train.shape[1]),
        "rf": {"threshold": float(thr_rf), "accuracy": rf_meta["accuracy"], "auc": rf_meta["auc"]},
        "nn": {"threshold": float(thr_nn), "accuracy": nn_meta["accuracy"], "auc": nn_meta["auc"]},
        "nn_history": nn_history,
    }

    with open(out_dir / "results" / "summary.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2)

    summary_path = out_dir / "results" / "training_summary.json"
    print(f"[INFO] Saved outputs to: {out_dir}")
    print(f"[INFO] Summary can be found at: {summary_path}")
    return summary
